{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os  \n",
    "from scipy.ndimage import filters  \n",
    "from PIL import Image  \n",
    "from pylab import *  \n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_gradients(img):\n",
    "    \"\"\" Compute the gradient of x, y, digonal 45 and \n",
    "        digonal 135 directions. \"\"\"\n",
    "    \n",
    "    rows, cols = img.shape\n",
    "    Ix2 = np.zeros((rows, cols))\n",
    "    Iy2 = np.zeros((rows, cols))\n",
    "    Ixy = np.zeros((rows, cols))\n",
    "    Iu45 = np.zeros((rows, cols))\n",
    "    Iv135 = np.zeros((rows, cols))\n",
    "    # box filter for x direction\n",
    "    fx = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "    # box filter for y direction\n",
    "    fy = np.array([[-1, -1, -1],\n",
    "                   [0,  0,  0, ],\n",
    "                   [1,  1,  1]])\n",
    "    # calculate the gradient of four directions for each pixel\n",
    "    for r in range(1, rows-1):\n",
    "        for c in range(1, cols-1):\n",
    "            p_m = img[r-1:r+2, c-1:c+2]\n",
    "            Ix = (p_m * fx).sum()\n",
    "            Iy = (p_m * fy).sum()\n",
    "            Ix2[r,c] = Ix * Ix\n",
    "            Iy2[r,c] = Iy * Iy\n",
    "            Ixy[r,c] = Ix * Iy\n",
    "            Iu45[r,c] = pow(img.item(r+1, c-1) - img.item(r-1, c+1), 2)\n",
    "            Iv135[r,c] = pow(img.item(r-1, c-1) - img.item(r+1, c+1), 2)\n",
    "    return Ix2, Iy2, Ixy, Iu45, Iv135\n",
    "\n",
    "def gauss_filter(img, delta=3):\n",
    "    \"\"\" A simiple gaussian filter but slooow\"\"\"\n",
    "    \n",
    "    rows, cols = img.shape\n",
    "    filter_out = np.zeros((rows,cols), dtype = float)\n",
    "    # the distance map\n",
    "    kernel = np.array([[2., 1., 2.],\n",
    "                       [1., 0., 1.],\n",
    "                       [2., 1., 2.]])\n",
    "    # gaussican kernel\n",
    "    kernel = np.exp(-kernel / (2*(delta**2)))\n",
    "    # nomalization\n",
    "    kernel = kernel / kernel.sum()\n",
    "    \n",
    "    # Not consider about the border!\n",
    "    for r in range(1, rows-1):\n",
    "        for c in range(1, cols-1):\n",
    "            filter_out[r,c] = (img[(r-1):(r+2), (c-1):(c+2)] * kernel).sum()\n",
    "\n",
    "    return filter_out\n",
    "\n",
    "def detect_harris_Kpoint(Ix2, Iy2, Ixy, K=4):\n",
    "    \"\"\" Dectect K*K harris points, each for one block.\"\"\"\n",
    "    \n",
    "    key_pts = np.zeros((K**2,3), dtype=int)\n",
    "    # get the filtered gradients\n",
    "    Ix2 = gauss_filter(Ix2)\n",
    "    Iy2 = gauss_filter(Iy2)\n",
    "    Ixy2 = gauss_filter(Ixy*Ixy)\n",
    "    \n",
    "    # obtain the harris response\n",
    "    R = (Ix2*Iy2 - Ixy2) / (Ix2 + Iy2 + 0.0001)\n",
    "    \n",
    "    # set the threshold, but it is not useful here\n",
    "    thresh_hold = R.max()*0.05\n",
    "    R[R < thresh_hold] = 0\n",
    "    rows, cols = R.shape\n",
    "    # block size\n",
    "    blockr = int(rows / K)\n",
    "    blockc = int(cols / K)\n",
    "\n",
    "    # get one key point with strongest response for each block\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            for r in range(blockr):\n",
    "                for c in range(blockc):\n",
    "                    pixel_r = i*blockr + r\n",
    "                    pixel_c = j*blockc + c\n",
    "                    pt_index = i*K +j\n",
    "                    if R[pixel_r,pixel_c] > key_pts[pt_index,2]:\n",
    "                        key_pts[pt_index,0] = pixel_r\n",
    "                        key_pts[pt_index,1] = pixel_c\n",
    "                        key_pts[pt_index,2] = R[pixel_r,pixel_c]\n",
    "    return key_pts, R\n",
    "\n",
    "def compute_harris_response(im, sigma=3):  \n",
    "    \"\"\" Compute the Harris corner detector response function  \n",
    "        for each pixel in a graylevel image. \"\"\"  \n",
    "  \n",
    "    # derivatives  \n",
    "    imx = np.zeros(im.shape)  \n",
    "    filters.gaussian_filter(im, (sigma, sigma), (0, 1), imx)  \n",
    "    imy = np.zeros(im.shape)  \n",
    "    filters.gaussian_filter(im, (sigma, sigma), (1, 0), imy)  \n",
    "  \n",
    "    # compute components of the Harris matrix  \n",
    "    Wxx = filters.gaussian_filter(imx * imx, sigma)  \n",
    "    Wxy = filters.gaussian_filter(imx * imy, sigma)  \n",
    "    Wyy = filters.gaussian_filter(imy * imy, sigma)  \n",
    "  \n",
    "    # determinant and trace  \n",
    "    Wdet = Wxx * Wyy - Wxy ** 2  \n",
    "    Wtr = Wxx + Wyy + 0.00001\n",
    "  \n",
    "    return Wdet / Wtr\n",
    "\n",
    "def get_harris_points(harrisim, min_dist=80, threshold=0.05):  \n",
    "    \"\"\" Return corners from a Harris response image  \n",
    "        min_dist is the minimum number of pixels separating  \n",
    "        corners and image boundary. \"\"\"  \n",
    "  \n",
    "    # find top corner candidates above a threshold  \n",
    "    corner_threshold = harrisim.max() * threshold  \n",
    "    harrisim_t = (harrisim > corner_threshold) * 1  \n",
    "  \n",
    "    # get coordinates of candidates  \n",
    "    coords = np.array(harrisim_t.nonzero()).T  \n",
    "  \n",
    "    # ...and their values  \n",
    "    candidate_values = [harrisim[c[0], c[1]] for c in coords]  \n",
    "  \n",
    "    # sort candidates (reverse to get descending order)  \n",
    "    index = argsort(candidate_values)[::-1]  \n",
    "  \n",
    "    # store allowed point locations in array  \n",
    "    allowed_locations = np.zeros(harrisim.shape)  \n",
    "    allowed_locations[min_dist:-min_dist, min_dist:-min_dist] = 1  \n",
    "  \n",
    "    # select the best points taking min_distance into account  \n",
    "    filtered_coords = []\n",
    "    for i in index:  \n",
    "        if allowed_locations[coords[i, 0], coords[i, 1]] == 1:  \n",
    "            filtered_coords.append(coords[i])  \n",
    "            allowed_locations[(coords[i, 0] - min_dist):(coords[i, 0] + min_dist),  \n",
    "            (coords[i, 1] - min_dist):(coords[i, 1] + min_dist)] = 0  \n",
    "  \n",
    "    return filtered_coords  \n",
    "\n",
    "def get_acculmulated_histgram(in_img):\n",
    "    \"\"\" Caculate the histogram of four gradient maps of four directions.\"\"\"\n",
    "    \n",
    "    rows, cols = in_img.shape\n",
    "    hori_histogram = np.zeros(cols, dtype = int)\n",
    "    ver_histogram = np.zeros(rows, dtype = int)\n",
    "    dia_45_histogram = np.zeros(rows + cols - 1, dtype = int)\n",
    "    length45 = np.zeros(rows + cols - 1, dtype = int)\n",
    "    dia_135_histogram = np.zeros(rows + cols - 1, dtype = int)\n",
    "    length135 = np.zeros(rows + cols - 1, dtype = int)\n",
    "    Ix2, Iy2, Ixy, Iu45, Iv135 = get_gradients(in_img)\n",
    "    \n",
    "    # get the harris points using the gradient map\n",
    "    key_points, R = detect_harris_Kpoint(Ix2, Iy2, Ixy)\n",
    "    \n",
    "    # fill the histogram of the gradient maps\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            hori_histogram[c] += Ix2[r,c] \n",
    "            ver_histogram[r] += Iy2[r,c]\n",
    "            index135 = r + c\n",
    "            index45 = rows - r + c - 1\n",
    "            dia_45_histogram[index45] += Iu45[r,c]\n",
    "            length45[index45] += 1\n",
    "            dia_135_histogram[index135] += Iv135[r,c]\n",
    "            length135[index135] += 1\n",
    "    \n",
    "    # normalize the histogram with the pixel number of each bin\n",
    "    hori_histogram = hori_histogram / rows\n",
    "    ver_histogram = ver_histogram / cols\n",
    "    dia_45_histogram = dia_45_histogram / length45\n",
    "    dia_135_histogram = dia_135_histogram / length135\n",
    "    \n",
    "#     plt.subplot(221),plt.plot([i for i in range(cols)], hori_histogram),plt.title('hori_')\n",
    "#     plt.subplot(222),plt.plot([i for i in range(rows)], ver_histogram),plt.title('ver_')\n",
    "#     plt.subplot(223),plt.plot([i for i in range(rows + cols - 1)], dia_45_histogram),plt.title('dia_45_')\n",
    "#     plt.subplot(224),plt.plot([i for i in range(rows + cols - 1)], dia_135_histogram),plt.title('dia_135_')\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.imshow(in_img, cmap=plt.cm.gray_r)\n",
    "#     plt.scatter(key_points[:,1], key_points[:,0], color='red')\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.imshow(Ix2, cmap=plt.cm.gray_r)\n",
    "#     plt.show()\n",
    "#     plt.imshow(Iy2, cmap=plt.cm.gray_r)\n",
    "#     plt.show()\n",
    "#     plt.imshow(Iu45, cmap=plt.cm.gray_r)\n",
    "#     plt.show();\n",
    "#     plt.imshow(Iv135, cmap=plt.cm.gray_r)\n",
    "#     plt.show();\n",
    "    return hori_histogram, ver_histogram, dia_45_histogram, dia_135_histogram, key_points, R\n",
    "\n",
    "def get_xy_trans(histo_p1, histo_p2, maxoffset=40):\n",
    "    \"\"\" Compute the transformation using the histogram.\"\"\"\n",
    "    \n",
    "    translation = 0\n",
    "    # set a max number\n",
    "    max_conv = 1e10000\n",
    "    \n",
    "    # the offset that make (x-y)^2 minimum is the right transformation\n",
    "    for offset in range(-maxoffset, maxoffset+1):\n",
    "        conv = ((histo_p2[maxoffset:-1-maxoffset] - histo_p1[maxoffset + offset:-1-maxoffset + offset])**2).sum()\n",
    "        if conv < max_conv:\n",
    "            max_conv = conv\n",
    "            translation = offset\n",
    "    return translation\n",
    "\n",
    "def get_correspondence_pt(pre_img, pre_keys, rough_tx, rough_ty, in_img, in_keys):\n",
    "    \"\"\" Fing the point pair using the transformation roughly caculated by the convolution\n",
    "        of the histogram.\"\"\"\n",
    "    \n",
    "    pre_rows,pre_cols = pre_keys.shape\n",
    "    in_rows, in_cols = in_keys.shape\n",
    "    # the maximum number of point pairs\n",
    "    if pre_rows >= in_rows:\n",
    "        max_rows = pre_rows\n",
    "    else:\n",
    "        max_rows = in_rows\n",
    "        \n",
    "    pt_pairs = np.zeros((max_rows,4), dtype=int)\n",
    "    \n",
    "    # for each key point in pre image, find the nearest key point in the coming image, considering the rough transformation\n",
    "    pt_index = 0\n",
    "    for k in range(pre_rows):\n",
    "        min_dist = 10**100\n",
    "        for kk in range(in_rows):\n",
    "            dist = pow(pre_keys[k,0] - in_keys[kk,0] - rough_ty,2.0) + pow(pre_keys[k,1] - in_keys[kk,1] - rough_tx,2.0)\n",
    "            # the point pair should have the smallest distance and the distance should Not bigger than 5 pixel in each direction\n",
    "            if (dist < min_dist and dist < 50):\n",
    "                pt_pairs[pt_index, 0] = pre_keys[k,0]\n",
    "                pt_pairs[pt_index, 1] = pre_keys[k,1]\n",
    "                pt_pairs[pt_index, 2] = in_keys[kk,0] + rough_ty\n",
    "                pt_pairs[pt_index, 3] = in_keys[kk,1] + rough_tx\n",
    "                min_dist = dist\n",
    "                pt_index += 1\n",
    "                \n",
    "    print(\"the points pairs number: \", pt_index)\n",
    "    return pt_pairs[0:pt_index,:]\n",
    "\n",
    "def my_ransac_xy(all_pt_pairs):\n",
    "    \"\"\" Adjust the rough tx and ty using the random sample consensus method \"\"\"\n",
    "    \n",
    "    pt_nums,cols = all_pt_pairs.shape\n",
    "    # intial the adjusted transformation\n",
    "    adjust_tx = 0.\n",
    "    adjust_ty = 0.\n",
    "    batch_num = 8\n",
    "    # ... and if there are not enough numbers of point pairs return the \n",
    "    # initial number\n",
    "    if pt_nums < batch_num:\n",
    "        print(\"error: not enough point pairs\")\n",
    "        return adjust_tx, adjust_ty\n",
    "    \n",
    "    picked_pt = np.zeros((batch_num,cols), dtype=float)\n",
    "    selected_ones = []\n",
    "    # the maximum number of the iteration\n",
    "    max_iteration = 50\n",
    "    \n",
    "    iteration = 0\n",
    "    max_inliners = 0\n",
    "    final_tx = 0.\n",
    "    final_ty = 0.\n",
    "    \n",
    "    while 1:\n",
    "        pick_num = 0\n",
    "        while (pick_num < batch_num):\n",
    "            picked_one = np.random.randint(0,pt_nums-1)\n",
    "            # all the point pairs in the same batch should be different\n",
    "            if (picked_one in selected_ones):\n",
    "                continue\n",
    "            selected_ones.append(picked_one)\n",
    "            pick_num += 1\n",
    "        # save the picked ones\n",
    "        picked_pt[:,:] = all_pt_pairs[selected_ones,:]\n",
    "        selected_ones = []\n",
    "\n",
    "        # adjusted value is the average of the transformation of all the point pairs\n",
    "        adjust_tx = ((picked_pt[:,1] - picked_pt[:,3]).sum(axis=0) / batch_num)\n",
    "        adjust_ty = ((picked_pt[:,0] - picked_pt[:,2]).sum(axis=0) / batch_num)\n",
    "#         print(\"adjsted = \",adjust_tx,adjust_ty)\n",
    "        inliners = 0\n",
    "        for k in range(pt_nums):\n",
    "            newr = all_pt_pairs[k,2] + adjust_ty\n",
    "            newc = all_pt_pairs[k,3] + adjust_tx\n",
    "            # count the inner points\n",
    "            if ((pow(newr - all_pt_pairs[k,0],2) + pow(newc - all_pt_pairs[k,1],2)) < 3):\n",
    "                inliners += 1\n",
    "                \n",
    "#         print(\"inliners = \", inliners)\n",
    "        # if the amount of the inner points is big eough, return directly\n",
    "        if (inliners > pt_nums * 0.9):\n",
    "            return adjust_tx, adjust_ty\n",
    "        # or we will choose the ones with the most inners\n",
    "        if (inliners > max_inliners):\n",
    "            max_inliners = inliners\n",
    "            print(\"max_inliners = \", max_inliners)\n",
    "            final_tx = adjust_tx\n",
    "            final_ty = adjust_ty\n",
    "        \n",
    "        iteration += 1\n",
    "        # if the maximum iteration is reached, return the best ones found so far\n",
    "        if ((iteration > max_iteration)):\n",
    "            return final_tx, final_ty\n",
    "\n",
    "def get_H(pt_pairs):\n",
    "    \"\"\" Caculate the homography matrix using eigenvalue decomposition methond.\n",
    "        Input is point pairs, the number should be greater than 4.\"\"\"\n",
    "    \n",
    "    rows, cols = pt_pairs.shape\n",
    "    # the format of the point pair should be x1,y1,x2,y2 and the number should be at least 4.\n",
    "    if (cols != 4 or rows < 4):\n",
    "        print(\"the points pairs size is not right!\")\n",
    "        return\n",
    "    # rewrite the projective transformation in the form of Ah = 0\n",
    "    # and fullfil the A matrix with each point pair\n",
    "    A = np.zeros((2*rows,9), dtype = float)\n",
    "    for k in range(rows):\n",
    "        A[2*k,:] = np.array([pt_pairs[k,2], pt_pairs[k,3], 1., 0.,0.,0., \n",
    "                            -pt_pairs[k,0]* pt_pairs[k,2],\n",
    "                            -pt_pairs[k,0]* pt_pairs[k,3],\n",
    "                            -pt_pairs[k,0]])\n",
    "        A[2*k+1,:] = np.array([0.,0.,0.,\n",
    "                               pt_pairs[k,2], \n",
    "                               pt_pairs[k,3], 1., \n",
    "                              -pt_pairs[k,1]*pt_pairs[k,2],\n",
    "                              -pt_pairs[k,1]*pt_pairs[k,3],\n",
    "                              -pt_pairs[k,1]])\n",
    "    A = A / np.max(abs(A))\n",
    "    # eigenvalue decompositon\n",
    "    w,b = np.linalg.eig(np.dot(A.transpose(),A))\n",
    "    # fing the index of the minmum eigenvalue\n",
    "    min_index = np.where(w == np.min(w))\n",
    "    # and the eigenvector of the minmum eigenvalue is the H\n",
    "    H = b[:,min_index].reshape(3,3)\n",
    "    # set the last element of the H to 1\n",
    "    H = H / H[2,2]\n",
    "    return H\n",
    "\n",
    "def colinear(pt1,pt2,pt3):\n",
    "    \"\"\" Estimate the three points are colinear or not. \"\"\"\n",
    "    \n",
    "    vct1 = pt2 - pt1\n",
    "    vct2 = pt3 - pt1\n",
    "    cos_theta = ((vct1 * vct2).sum()) / (sqrt(np.dot(vct1,vct1.transpose()))*sqrt(np.dot(vct2,vct2.transpose())))\n",
    "    # if the angle of the two vector is near 0 or 180, we think they are colinear.\n",
    "    if (abs(cos_theta)) < 0.95:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def my_ransac_H(all_pt_pairs, thresh_hold = 2):\n",
    "    \"\"\" Using random sample consensus method to find the H with the most inners.\n",
    "        Input: the point pairs in the form of x1,y1,x2,y2\n",
    "        Output: H and inner point pairs\"\"\"\n",
    "    \n",
    "    pt_nums,cols = all_pt_pairs.shape\n",
    "    # each time use 4 point-pairs to caculate a H\n",
    "    batch_nums = 4\n",
    "    H_final = np.array([[1.,0.,0.],\n",
    "                        [0.,1.,0.],\n",
    "                        [0.,0.,1.]])\n",
    "    in_liners = np.zeros((pt_nums,4), dtype=float)\n",
    "    \n",
    "    if pt_nums < batch_nums:\n",
    "        print(\"error: not enough point pairs\")\n",
    "        return H_final, in_liners\n",
    "    \n",
    "    picked_pt = np.zeros((batch_nums,cols), dtype=float)\n",
    "    # the maximum number of iteration for the caculation of H\n",
    "    max_iteration = 100\n",
    "    # the maximum number of iteration to pick the four point-pairs\n",
    "    max_iteration_pickpt = 50\n",
    "    \n",
    "    iteration = 0\n",
    "    max_inliners = 0\n",
    "\n",
    "    while 1:\n",
    "        pick_num = 0\n",
    "        iteration_pick = 0\n",
    "        selected_ones = []\n",
    "        while (pick_num < batch_nums):\n",
    "            iteration_pick += 1\n",
    "            # can not find enough non-colinear points, return the best one being found so far\n",
    "            if (iteration_pick > max_iteration_pickpt):\n",
    "                return H_final,in_liners\n",
    "            picked_one = np.random.randint(0,pt_nums-1)\n",
    "            # all the points should be different\n",
    "            if (picked_one in selected_ones):\n",
    "                continue\n",
    "            # every three of them should Not be colinear\n",
    "            if (pick_num == 2):\n",
    "                p1 = all_pt_pairs[selected_ones[0],0:2]\n",
    "                p2 = all_pt_pairs[selected_ones[1],0:2]\n",
    "                p3 = all_pt_pairs[picked_one,0:2]\n",
    "                if (colinear(p1,p2,p3)):\n",
    "                    continue\n",
    "            if (pick_num == 3):\n",
    "                p1 = all_pt_pairs[selected_ones[0],0:2]\n",
    "                p2 = all_pt_pairs[selected_ones[1],0:2]\n",
    "                p3 = all_pt_pairs[selected_ones[2],0:2]\n",
    "                p4 = all_pt_pairs[picked_one,0:2]\n",
    "                if((colinear(p1,p2,p4)) or (colinear(p1,p3,p4)) or (colinear(p2,p3,p4))):\n",
    "                    continue\n",
    "            selected_ones.append(picked_one)\n",
    "            pick_num += 1\n",
    "        # set the picked ones\n",
    "        picked_pt[:,:] = all_pt_pairs[selected_ones,:]\n",
    "        selected_ones = []\n",
    "        # ... and use them to caculate the H\n",
    "        H_temp = get_H(picked_pt)\n",
    "        \n",
    "        inliners = 0\n",
    "        selected_ones = []\n",
    "        for k in range(pt_nums):\n",
    "            newxy = np.dot(H_temp, [all_pt_pairs[k,2],all_pt_pairs[k,3],1.])\n",
    "            newxy = newxy / newxy[2]\n",
    "            # pick out all the inners\n",
    "            if ((pow(newxy[0] - all_pt_pairs[k,0],2) + pow(newxy[1] - all_pt_pairs[k,1],2)) < thresh_hold):\n",
    "                selected_ones.append(k)\n",
    "                inliners += 1\n",
    "        # update the inner set\n",
    "        in_subset = all_pt_pairs[selected_ones,:]\n",
    "        print(\"inliners = \", inliners)\n",
    "        # if the amount of the inners is big enough, return directly\n",
    "        if (inliners > pt_nums*0.9):\n",
    "            return H_temp, in_subset\n",
    "        # store the best H and its inners\n",
    "        if (inliners > max_inliners):\n",
    "            max_inliners = inliners\n",
    "            print(\"max_inliners = \", max_inliners)\n",
    "            H_final = H_temp\n",
    "            in_liners[0:inliners,:] = in_subset\n",
    "        # if the inners are good enough, update the iteration number or we think this iteration is false\n",
    "        if inliners > batch_nums/2:\n",
    "            iteration += 1\n",
    "        # the iteration is over, return the best H having being found so far and its inners\n",
    "        if ((iteration > max_iteration)):\n",
    "            return H_final, in_liners\n",
    "        \n",
    "def seam_cut(pre_img, in_img, tx, ty):\n",
    "    \"\"\" Caculate the best seam line. Not used now.\"\"\"\n",
    "    \n",
    "    if (tx < 0):\n",
    "        print(\"tx < 0, out!\")\n",
    "        return\n",
    "    rows, cols = in_img.shape\n",
    "    cut_line = np.zeros((rows,2), dtype= int)\n",
    "    \n",
    "    if (ty < 0):\n",
    "        overlap_in = pre_img[0:rows+ty,tx::]\n",
    "        overlap_dst = in_img[0-ty::,0:cols-tx]\n",
    "    if (ty >= 0):\n",
    "        overlap_in = pre_img[ty::,tx::]\n",
    "        overlap_dst = in_img[0:rows-ty,0:cols-tx]\n",
    "    rows, cols = overlap_in.shape\n",
    "    print(overlap_in.shape)\n",
    "    print(overlap_dst.shape)\n",
    "    \n",
    "    overlap_in.astype(float)\n",
    "    overlap_dst.astype(float)\n",
    "    print(overlap_in.dtype)\n",
    "    dis = ((overlap_in - overlap_dst)**2.)\n",
    "    print(dis.dtype)\n",
    "    \n",
    "    for i in range(1,rows):\n",
    "        for j in range(cols):\n",
    "            if (j == 0):\n",
    "                min_neibor = dis[i-1,j]\n",
    "                if (dis[i-1, j+1] <= min_neibor):\n",
    "                    min_neibor = dis[i-1,j+1]\n",
    "                dis[i,j] = dis[i,j] + min_neibor\n",
    "                continue\n",
    "                \n",
    "            if (j == cols-1):\n",
    "                min_neibor = dis[i-1,j]\n",
    "                if (dis[i-1, j-1] <= min_neibor):\n",
    "                    min_neibor = dis[i-1,j-1]\n",
    "                dis[i,j] = dis[i,j] + min_neibor\n",
    "                continue\n",
    "                \n",
    "            min_neibor = dis[i-1,j]\n",
    "            if (dis[i-1,j+1] < min_neibor):\n",
    "                min_neibor = dis[i-1,j+1]\n",
    "            if (dis[i-1,j-1] < min_neibor):\n",
    "                min_neibor = dis[i-1,j-1]\n",
    "#             if (dis[i-1,j+1] < min_neibor):\n",
    "#                 min_neibor = dis[i-1,j+1]\n",
    "            dis[i,j] = dis[i,j] + min_neibor\n",
    "    \n",
    "    min_err = dis[int(rows/2), int(cols/2)]\n",
    "    min_c = 0\n",
    "    for c in range(cols-2,1,-1):\n",
    "        if (dis[rows-1,c] < min_err):\n",
    "            min_err = dis[rows-1,c]\n",
    "            min_c = c\n",
    "    if(ty > 0):\n",
    "        cut_r = rows-1\n",
    "    else:\n",
    "        cut_r = rows-1-ty\n",
    "    cut_line[cut_r, 0] = rows-1\n",
    "    cut_line[cut_r, 1] = min_c\n",
    "    \n",
    "    for r in range(rows-2,-1,-1):\n",
    "        min_mid = min_c\n",
    "        \n",
    "        if(min_c == 0):\n",
    "            min_err = dis[r, min_c]\n",
    "            if (dis[r, min_c+1] <= min_err):\n",
    "                min_mid = min_c +1\n",
    "            min_c = min_mid\n",
    "            \n",
    "            if(ty > 0):\n",
    "                cut_r = r\n",
    "            else:\n",
    "                cut_r = r-ty\n",
    "            cut_line[cut_r, 0] = cut_r\n",
    "            cut_line[cut_r, 1] = min_c\n",
    "            continue\n",
    "            \n",
    "        if(min_c == cols-1):\n",
    "            min_err = dis[r, min_c]\n",
    "            if (dis[r, min_c-1] <= min_err):\n",
    "                min_mid = min_c -1\n",
    "            min_c = min_mid\n",
    "            \n",
    "            if(ty > 0):\n",
    "                cut_r = r\n",
    "            else:\n",
    "                cut_r = r-ty\n",
    "            cut_line[cut_r, 0] = cut_r\n",
    "            cut_line[cut_r, 1] = min_c\n",
    "            continue\n",
    "        \n",
    "        min_err = dis[r, min_c]\n",
    "        if (dis[r, min_c-1] < min_err):\n",
    "            min_mid = min_c-1\n",
    "            min_err = dis[r, min_c-1]\n",
    "        if (dis[r, min_c+1] < min_err):\n",
    "            min_mid = min_c+1\n",
    "            min_err = dis[r, min_c+1]\n",
    "        min_c = min_mid\n",
    "        if(ty > 0):\n",
    "            cut_r = r\n",
    "        else:\n",
    "            cut_r = r-ty\n",
    "        cut_line[cut_r, 0] = cut_r\n",
    "        cut_line[cut_r, 1] = min_c\n",
    "\n",
    "    return cut_line\n",
    "\n",
    "def stitch_to_overlap(final_pano, next_img, Ht):\n",
    "    \"\"\" Project the coming image to the final panorama image using the caculated homography\n",
    "        matrix and directly replace the pixel in the panorama.\"\"\"\n",
    "    \n",
    "    rows_in,cols_in = next_img.shape\n",
    "    rows_dst,cols_dst = final_pano.shape\n",
    "    for r in range(rows_in):\n",
    "        for c in range(cols_in):\n",
    "            # caculate the final coordinate of each pixel in the current image\n",
    "            newxy = np.dot(Ht, [r,c,1])\n",
    "            newxy = newxy / newxy[2]\n",
    "            if ((newxy[0] < 0) or (newxy[0] > rows_dst-1) or (newxy[1] < 0) or (newxy[1] > cols_dst-1)):\n",
    "                continue\n",
    "            # set the pixel value in the panorama image\n",
    "            final_pano[int(newxy[0]), int(newxy[1])] = next_img[r,c]\n",
    "    return final_pano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read the video\n",
    "cap = cv2.VideoCapture(\"E:\\\\alls\\\\VID_20100115_195346.3gp\")\n",
    "# get the total number of the frames\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"the length of the video is :\", length)\n",
    "# get one frame\n",
    "ret, first_frame = cap.read()\n",
    "# ... and turn it into gray style\n",
    "gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "# ... and rotate it into the regular angle\n",
    "gray = cv2.flip(cv2.flip(np.rot90(gray),0),1)\n",
    "\n",
    "# initialize the panorama, its width is N times the width of the video\n",
    "final_pano = np.zeros((gray.shape[0], gray.shape[1] * 3), dtype=int)\n",
    "\n",
    "# obtain the harris points\n",
    "harris_res = compute_harris_response(gray)\n",
    "all_harris = get_harris_points(harris_res)\n",
    "keys = np.array(all_harris)\n",
    "print(keys.shape)\n",
    "# ... and the histogram of four directions\n",
    "HH1, VH1, DH45, DH135, my_keypts, R = get_acculmulated_histgram(gray)\n",
    "H_total = np.array([[1.,0.,0.],\n",
    "                    [0.,1.,0.],\n",
    "                    [0.,0.,1.]])\n",
    "for i in range(int(length/2)):\n",
    "    print(\"the iteration: \", i)\n",
    "    # do one caculation every two frames\n",
    "    for s in range(2):\n",
    "        ret1, frame1 = cap.read()\n",
    "    \n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray1 = cv2.flip(cv2.flip(np.rot90(gray1),0),1)\n",
    "\n",
    "    harris_res = compute_harris_response(gray1)\n",
    "    all_harris = get_harris_points(harris_res)\n",
    "    keys2 = np.array(all_harris)    \n",
    "    print(keys2.shape)\n",
    "    \n",
    "    HH2, VH2, DH452, DH1352, knone, Rnone = get_acculmulated_histgram(gray1)\n",
    "\n",
    "    tx = get_xy_trans(HH1, HH2)\n",
    "    # we just allow the camera to move forword for now\n",
    "    if tx < 0:\n",
    "        continue\n",
    "    ty = get_xy_trans(VH1, VH2)\n",
    "    tu = get_xy_trans(DH45, DH452)\n",
    "    tv = get_xy_trans(DH135, DH1352)\n",
    "    # adjust the x, y transformation using the diagonal ones\n",
    "    tx = (tx + 1.414/2.0 * tu + 1.414/2. * tv) / 2.\n",
    "    ty = (ty - 1.414/2.0 * tu + 1.414/2. * tv) / 2.\n",
    "    print(\"tx =\",tx)\n",
    "    print(\"ty =\",ty)\n",
    "    \n",
    "    # obtain the point pairs\n",
    "    points = get_correspondence_pt(gray, keys, tx, ty, gray1, keys2)\n",
    "    # ... and adjust the values with these points\n",
    "    adjusted_tx, adjusted_ty = my_ransac_xy(points)\n",
    "    print(\"adjusted_tx =\",adjusted_tx)\n",
    "    print(\"adjusted_ty =\",adjusted_ty)\n",
    "    tx = tx + adjusted_tx\n",
    "    ty = ty + adjusted_ty\n",
    "    print(\"tx =\",tx)\n",
    "    print(\"ty =\",ty)\n",
    "    # update the coordinate of the new point pairs\n",
    "    points[:,2] = points[:,2] + adjusted_ty\n",
    "    points[:,3] = points[:,3] + adjusted_tx\n",
    "    \n",
    "    # caculate the error using tx, ty only\n",
    "    error_xy = (pow(points[:,0]-points[:,2],2)+pow(points[:,1]-points[:,3],2)).sum()\n",
    "    print(\"error after tx,ty:\",error_xy)\n",
    "    \n",
    "    # obtain the homography matrix \n",
    "    H_new, subset = my_ransac_H(points,3)\n",
    "    # ... and improve the result using the inners and a low threshold\n",
    "    H_new, subset = my_ransac_H(subset,1.5)\n",
    "    \n",
    "    # plot the point set \n",
    "    plt.scatter(points[:,1], points[:,0], color='red')\n",
    "    point_trans = np.zeros((3,points.shape[0]))\n",
    "    point_trans[0,:] = points[:,2]\n",
    "    point_trans[1,:] = points[:,3]\n",
    "    point_trans[2,:] = 1\n",
    "    point_trans = np.dot(H_new, point_trans)\n",
    "    point_trans[0,:] = point_trans[0,:] / point_trans[2,:]\n",
    "    point_trans[1,:] = point_trans[1,:] / point_trans[2,:]\n",
    "    plt.scatter(point_trans[1,:], point_trans[0,:], marker = 'x', color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    # caculate the error using tx,ty and H\n",
    "    error_xyH = (pow(points[:,0]-point_trans[0,:],2)+pow(points[:,1]-point_trans[1,:],2)).sum()\n",
    "    print(\"error after tx,ty,H:\",error_xyH)\n",
    "    \n",
    "    # it should be lower, but if the error increase for some reason, we dismiss it\n",
    "    if error_xyH > error_xy:\n",
    "        H_new = np.array([[1.,0.,0.],\n",
    "                          [0.,1.,0.],\n",
    "                          [0.,0.,1.]])\n",
    "    # multiply tx, ty, and H, obtain the total transformation for the current frame\n",
    "    transH = np.array([[1., 0., ty],\n",
    "                       [0., 1., tx],\n",
    "                       [0., 0., 1.]])\n",
    "    H_new = np.dot(H_new, transH)\n",
    "    H_new = H_new / H_new[2,2]\n",
    "    # ... and accumulated with the previous one\n",
    "    H_total = np.dot(H_total, H_new)\n",
    "    H_total = H_total / H_total[2,2]\n",
    "    # stitch to the panorama\n",
    "    final_pano = stitch_to_overlap(final_pano, gray1, H_total)\n",
    "\n",
    "    HH1 = HH2\n",
    "    VH1 = VH2\n",
    "    DH45 = DH452\n",
    "    DH135 = DH1352\n",
    "    gray = gray1\n",
    "    keys = keys2\n",
    "    # save the panorame each time for debugging only\n",
    "    cv2.imwrite(\"final_pano-3.png\", final_pano)\n",
    "    \n",
    "#cv2.imwrite(\"final_pano-3.png\", final_pano)\n",
    "plt.imshow(final_pano, cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print((pow(pt_inliner[:,1] - pt_inliner[:,3], 2) + pow(pt_inliner[:,0] - pt_inliner[:,2], 2)).sum(axis=0))\n",
    "# pt_transed = (np.dot(R, pt_inliner[:,2:4].transpose()) + T).transpose()\n",
    "# print((pow(pt_inliner[:,1] - pt_transed[:,1], 2) + pow(pt_inliner[:,0] - pt_transed[:,0], 2)).sum(axis=0))\n",
    "\n",
    "# plt.scatter(keys[:,1], keys[:,0], color='red', marker='o')\n",
    "# plt.scatter(keys2[:,1], keys2[:,0], color='blue', marker='x')\n",
    "# plt.show()\n",
    "# plt.scatter(pt_inliner[:,1], pt_inliner[:,0], color='red', marker='o')\n",
    "# plt.scatter(pt_inliner[:,3], pt_inliner[:,2], color='blue', marker='x')\n",
    "# plt.show()\n",
    "# plt.scatter(pt_inliner[:,1], pt_inliner[:,0], color='red', marker='o')\n",
    "# plt.scatter(pt_transed[:,1], pt_transed[:,0], color='blue', marker='x')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow((gray+gray1)/2, cmap=plt.cm.gray)\n",
    "# plt.show();\n",
    "# rows, cols = gray1.shape\n",
    "# Ttotal = T + np.dot(R, [[ty],[tx]])\n",
    "# gray_TR = np.zeros((rows,cols), dtype=int)\n",
    "# print(Ttotal)\n",
    "# for r in range(rows):\n",
    "#     for c in range(cols):\n",
    "#         newrc = np.dot(R, [[r],[c]]) + Ttotal\n",
    "#         newrc = np.round(newrc)\n",
    "#         if ((newrc[0] > 0) and (newrc[0] < rows) and (newrc[1] > 0) and (newrc[1] < cols)):\n",
    "#             gray_TR[int(newrc[0]), int(newrc[1])] = gray1[r,c]            \n",
    "\n",
    "# print(rows,cols)\n",
    "# print(newrc)\n",
    "# plt.imshow((gray+gray_TR)/2, cmap=plt.cm.gray)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
